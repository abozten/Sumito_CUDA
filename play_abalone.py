import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import copy
import time
from typing import List, Tuple

# Import the Abalone game implementation
from abalone_game import AbaloneGame, Player

# Check for CUDA GPU availability and set device
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using CUDA GPU device: {device}")
else:
    device = torch.device("cpu")
    print("CUDA GPU not available, using CPU")


class DQNModel(nn.Module):
    """
    Deep Q-Network for Abalone.
    Input: Board state representation
    Output: Q-values for each possible action index. Action indices are generated by valid moves
    """

    def __init__(self, input_channels=3, board_size=9):  # Removed num_actions
        super(DQNModel, self).__init__()

        # Enhanced network architecture with residual connections
        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # Residual block
        self.res_conv1 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.res_bn1 = nn.BatchNorm2d(128)
        self.res_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.res_bn2 = nn.BatchNorm2d(128)

        # Value and advantage streams (Dueling DQN architecture)
        flat_size = 128 * board_size * board_size

        # Value stream
        self.value_stream = nn.Sequential(
            nn.Linear(flat_size, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

        # Advantage stream - Dynamically sized output
        self.advantage_stream = nn.Sequential(
            nn.Linear(flat_size, 512),
            nn.ReLU(),
            nn.Linear(512, 128)  # Reduced output size
        )

        self.relu = nn.ReLU()

    def forward(self, x, num_actions):
        # Convolutional layers with batch normalization
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.relu(self.bn3(self.conv3(x)))

        # Residual block
        residual = x
        x = self.relu(self.res_bn1(self.res_conv1(x)))
        x = self.res_bn2(self.res_conv2(x))
        x = self.relu(x + residual)  # Skip connection

        # Flatten
        x = x.view(x.size(0), -1)

        # Dueling DQN: split into value and advantage streams
        value = self.value_stream(x)
        advantage = self.advantage_stream(x)

        # Dynamically create the final layer based on the number of actions
        action_logits = nn.Linear(128, num_actions).to(x.device)(advantage)  # Adjust this layer's dimensions

        # Combine value and advantage to get Q-values
        q_values = value + (action_logits - action_logits.mean(dim=1, keepdim=True))

        return q_values


class PrioritizedReplayBuffer:
    """Prioritized Experience Replay buffer for more efficient learning."""

    def __init__(self, capacity=50000, alpha=0.6, beta=0.4, beta_increment=0.0001):
        self.capacity = capacity
        self.buffer = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.alpha = alpha  # How much prioritization to use (0 = none, 1 = full)
        self.beta = beta  # Importance sampling correction (0 = no correction, 1 = full)
        self.beta_increment = beta_increment
        self.max_priority = 1.0
        self.next_idx = 0

    def add(self, state, action_idx, reward, next_state, done):
        """Add experience to buffer with maximum priority to ensure it's sampled."""
        experience = (state, action_idx, reward, next_state, done)

        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
        else:
            self.buffer[self.next_idx] = experience

        # New experiences get max priority to ensure they're sampled
        self.priorities[self.next_idx] = self.max_priority
        self.next_idx = (self.next_idx + 1) % self.capacity

    def sample(self, batch_size):
        """Sample a batch of experiences based on their priorities."""
        if len(self.buffer) == 0:
            return [], [], []

        # Increase beta for more accurate bias correction over time
        self.beta = min(1.0, self.beta + self.beta_increment)

        # Calculate sampling probabilities from priorities
        priorities = self.priorities[:len(self.buffer)]
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()

        # Sample indices based on priorities
        indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), p=probabilities)

        # Get samples
        samples = [self.buffer[idx] for idx in indices]
        states, action_indices, rewards, next_states, dones = zip(*samples)

        # Calculate importance sampling weights
        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)
        weights /= weights.max()  # Normalize weights

        return (
            np.array(states),
            np.array(action_indices),
            np.array(rewards),
            np.array(next_states),
            np.array(dones),
            indices,
            weights
        )

    def update_priorities(self, indices, errors):
        """Update priorities based on TD errors."""
        for idx, error in zip(indices, errors):
            self.priorities[idx] = error + 1e-5  # Small constant to prevent zero priority
            self.max_priority = max(self.max_priority, self.priorities[idx])

    def __len__(self):
        return len(self.buffer)


class AbaloneAI:
    """AI agent for playing Abalone."""

    def __init__(self, player: Player, model_path=None): #Removed training only parameters
        self.player = player

        # Initialize the models and move them to the appropriate device
        self.model = DQNModel().to(device)

        if model_path: #Load model is available
            self.load_model(model_path)
        self.model.eval() #Set model to evaluation model

    def select_action(self, game: AbaloneGame):
        """Select an action using the trained model (no exploration)."""
        valid_moves = game.get_valid_moves()
        if not valid_moves:
            return None

        # Convert state to tensor and move to device
        state = game.get_state_representation()
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dimension
        num_actions = len(valid_moves)  # Dynamically determine the number of actions

        # Select the best action according to the model
        with torch.no_grad():
            q_values = self.model(state_tensor, num_actions)
            action_idx = torch.argmax(q_values).item()  # Select the action with the highest Q-value

        # Get the actual move
        action = valid_moves[action_idx]

        return action
    def load_model(self, path):
        """Load the model from disk with training stats."""
        checkpoint = torch.load(path, map_location=device)
        self.model.load_state_dict(checkpoint['model_state_dict'])

def get_human_move(game: AbaloneGame):
    """Get a move from the human player."""
    game.display()
    valid_moves = game.get_valid_moves()

    if not valid_moves:
        print("No valid moves available.")
        return None

    print("\nAvailable moves:")
    for i, (line, direction) in enumerate(valid_moves):
        dir_names = ["NE", "E", "SE", "SW", "W", "NW"]
        print(f"{i}: Move line {line} in direction {dir_names[direction]}")

    while True:
        try:
            move_index = int(input(f"Select move (0-{len(valid_moves) - 1}): "))
            if 0 <= move_index < len(valid_moves):
                break
            else:
                print(f"Please enter a number between 0 and {len(valid_moves) - 1}")
        except ValueError:
            print("Please enter a valid number")

    return valid_moves[move_index]

def play_vs_ai(ai: AbaloneAI, human_player: Player):
    """Play a game against the trained AI."""
    game = AbaloneGame()
    ai_player = ai.player
    human_turn = True if human_player == Player.BLACK else False
    print(f"You are playing as {human_player.name}.")
    while not game.game_over:
        game.display()
        if (game.current_player == human_player):
            #Human turn
            action = get_human_move(game)
            if action is None:
                print("No valid moves. Game Over")
                break
            line, direction = action
            game.make_move(line, direction)
        else: #Ai turn
             print(f"\nAI ({ai_player.name}) is thinking...")
             action = ai.select_action(game)
             if action is None:
                print("No valid moves. Game Over")
                break
             line, direction = action
             game.make_move(line, direction) #Apply action
             print(f"AI ({ai_player.name}) moved line {line} in direction {direction}.")

        if game.turn_count > 200:
          game.game_over = True
          game.winner = None
          print ("Game lasted to long so Draw Game")
          break

    #Game over

    game.display()
    if game.winner:
        print(f"Game over! {game.winner.name} wins!")
    else:
        print("Game over! It's a draw!")
def main():
    # Load existing model
    model_path = input("Enter path to the trained AI model (leave empty for default 'black_ai_final.pth'): ")
    model_path = model_path or "black_ai_final.pth" #Use default

    try:
        ai = AbaloneAI(Player.WHITE, model_path) # or Player.BLACK, depending on what AI was trained as
        print("AI model loaded successfully.")
    except Exception as e:
        print(f"Error loading model: {e}")
        return

    #Ask the human what player they want
    human_player_str = input("Do you want to play as black(b) or white(w):")
    human_player_str = human_player_str.lower()
    if human_player_str == 'b':
      human_player = Player.BLACK
    elif human_player_str == 'w':
      human_player = Player.WHITE
    else: #Pick at random
      human_player = random.choice([Player.WHITE, Player.BLACK])
      print ("Invalid input so assigned" + human_player.name)

    play_vs_ai(ai, human_player)

if __name__ == "__main__":
    main()